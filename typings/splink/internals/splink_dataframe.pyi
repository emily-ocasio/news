"""
This type stub file was generated by pyright.
"""

from abc import ABC, abstractmethod, abstractproperty
from typing import Any, Optional, TYPE_CHECKING
from duckdb import DuckDBPyRelation
from splink.internals.input_column import InputColumn
from splink.internals.database_api import DatabaseAPI

logger = ...
if TYPE_CHECKING:
    ...
class SplinkDataFrame(ABC):
    """Abstraction over dataframe to handle basic operations like retrieving data and
    retrieving column names, which need different implementations depending on whether
    it's a spark dataframe, sqlite table etc.
    Uses methods like `as_pandas_dataframe()` and `as_record_dict()` to retrieve data
    """
    def __init__(self, templated_name: str, physical_name: str, db_api: DatabaseAPI[Any], metadata: dict[str, Any] = ...) -> None:
        ...
    
    @abstractproperty
    def columns(self) -> list[InputColumn]:
        ...
    
    @property
    def columns_escaped(self): # -> list[str]:
        ...
    
    @abstractmethod
    def validate(self): # -> None:
        ...
    
    @property
    def physical_and_template_names_equal(self): # -> bool:
        ...
    
    def drop_table_from_database_and_remove_from_cache(self, force_non_splink_table: bool = ...) -> None:
        """Drops the table from the underlying database, and removes it
        from the (linker) cache.

        By default this will fail if the table is not one created by Splink,
        but this check can be overriden

        Examples:
            ```py
            df_predict = linker.inference.predict()
            df_predict.drop_table_from_database_and_remove_from_cache()
            # predictions table no longer in the database / cache
            ```
        Args:
            force_non_splink_table (bool, optional): If True, skip check if the
                table was created by Splink and always drop regardless. If False,
                only drop if table was created by Splink. Defaults to False.

        """
        ...
    
    def as_record_dict(self, limit: Optional[int] = ...) -> list[dict[str, Any]]:
        """Return the dataframe as a list of record dictionaries.

        This can be computationally expensive if the dataframe is large.

        Examples:
            ```py
            df_predict = linker.inference.predict()
            ten_edges = df_predict.as_record_dict(10)
            ```
        Args:
            limit (int, optional): If provided, return this number of rows (equivalent
            to a limit statement in SQL). Defaults to None, meaning return all rows

        Returns:
            list: a list of records, each of which is a dictionary
        """
        ...
    
    def as_pandas_dataframe(self, limit=...): # -> DataFrame:
        """Return the dataframe as a pandas dataframe.

        This can be computationally expensive if the dataframe is large.

        Args:
            limit (int, optional): If provided, return this number of rows (equivalent
                to a limit statement in SQL). Defaults to None, meaning return all rows

        Examples:
            ```py
            df_predict = linker.inference.predict()
            df_ten_edges = df_predict.as_pandas_dataframe(10)
            ```
        Returns:
            pandas.DataFrame: pandas Dataframe
        """
        ...
    
    def as_duckdbpyrelation(self, limit: Optional[int] = ...) -> DuckDBPyRelation:
        """Return the dataframe as a duckdbpyrelation.  Only available when using the
        DuckDB backend.

        Args:
            limit (int, optional): If provided, return this number of rows (equivalent
                to a limit statement in SQL). Defaults to None, meaning return all rows

        Returns:
            duckdb.DuckDBPyRelation: A DuckDBPyRelation object
        """
        ...
    
    def as_spark_dataframe(self) -> Any:
        """Return the dataframe as a spark dataframe.  Only available when using the
        Spark backend.

        Returns:
            spark.DataFrame: A Spark DataFrame
        """
        ...
    
    def to_parquet(self, filepath, overwrite=...):
        """Save the dataframe in parquet format.

        Examples:
            ```py
            df_predict = linker.inference.predict()
            df_predict.to_parquet("model_predictions.parquet", overwrite=True)
            ```
        Args:
            filepath (str): Filepath where csv will be saved.
            overwrite (bool, optional): If True, overwrites file if it already exists.
                Default is False.
        """
        ...
    
    def to_csv(self, filepath, overwrite=...):
        """Save the dataframe in csv format.

        Examples:
            ```py
            df_predict = linker.inference.predict()
            df_predict.to_csv("model_predictions.csv", overwrite=True)
            ```
        Args:
            filepath (str): Filepath where csv will be saved.
            overwrite (bool, optional): If True, overwrites file if it already exists.
                Default is False.
        """
        ...
    
    def check_file_exists(self, filepath): # -> None:
        ...
    


