"""
This type stub file was generated by pyright.
"""

from typing import TYPE_CHECKING
from splink.internals.database_api import AcceptableInputTableType
from splink.internals.splink_dataframe import SplinkDataFrame
from splink.internals.linker import Linker

if TYPE_CHECKING:
    ...
logger = ...
class LinkerTableManagement:
    """Register Splink tables against your database backend and manage the Splink cache.
    Accessed via `linker.table_management`.
    """
    def __init__(self, linker: Linker) -> None:
        ...
    
    def compute_tf_table(self, column_name: str) -> SplinkDataFrame:
        """Compute a term frequency table for a given column and persist to the database

        This method is useful if you want to pre-compute term frequency tables e.g.
        so that real time linkage executes faster, or so that you can estimate
        various models without having to recompute term frequency tables each time

        Examples:

            Real time linkage
            ```py
            linker = Linker(df, settings="saved_settings.json", db_api=db_api)
            linker.table_management.compute_tf_table("surname")
            linker.compare_two_records(record_left, record_right)
            ```
            Pre-computed term frequency tables
            ```py
            linker = Linker(df, db_api)
            df_first_name_tf = linker.table_management.compute_tf_table("first_name")
            df_first_name_tf.write.parquet("folder/first_name_tf")
            >>>
            # On subsequent data linking job, read this table rather than recompute
            df_first_name_tf = pd.read_parquet("folder/first_name_tf")
            linker.table_management.register_term_frequency_lookup(
                df_first_name_tf, "first_name"
            )

            ```


        Args:
            column_name (str): The column name in the input table

        Returns:
            SplinkDataFrame: The resultant table as a splink data frame
        """
        ...
    
    def invalidate_cache(self): # -> None:
        """Invalidate the Splink cache.  Any previously-computed tables
        will be recomputed.
        This is useful, for example, if the input data tables have changed.
        """
        ...
    
    def register_table_input_nodes_concat_with_tf(self, input_data: AcceptableInputTableType, overwrite: bool = ...) -> SplinkDataFrame:
        """Register a pre-computed version of the input_nodes_concat_with_tf table that
        you want to re-use e.g. that you created in a previous run.

        This method allows you to register this table in the Splink cache so it will be
        used rather than Splink computing this table anew.

        Args:
            input_data (AcceptableInputTableType): The data you wish to register. This
                can be either a dictionary, pandas dataframe, pyarrow table or a spark
                dataframe.
            overwrite (bool): Overwrite the table in the underlying database if it
                exists.

        Returns:
            SplinkDataFrame: An abstraction representing the table created by the sql
                pipeline
        """
        ...
    
    def register_table_predict(self, input_data, overwrite=...): # -> SplinkDataFrame:
        """Register a pre-computed version of the prediction table for use in Splink.

        This method allows you to register a pre-computed prediction table in the Splink
        cache so it will be used rather than Splink computing the table anew.

        Examples:
            ```py
            predict_df = pd.read_parquet("path/to/predict_df.parquet")
            predict_as_splinkdataframe = linker.table_management.register_table_predict(predict_df)
            clusters = linker.clustering.cluster_pairwise_predictions_at_threshold(
                predict_as_splinkdataframe, threshold_match_probability=0.75
            )
            ```

        Args:
            input_data (AcceptableInputTableType): The data you wish to register. This
                can be either a dictionary, pandas dataframe, pyarrow table, or a spark
                dataframe.
            overwrite (bool, optional): Overwrite the table in the underlying database
                if it exists. Defaults to False.

        Returns:
            SplinkDataFrame: An abstraction representing the table created by the SQL
                pipeline.
        """
        ...
    
    def register_term_frequency_lookup(self, input_data, col_name, overwrite=...): # -> SplinkDataFrame:
        """Register a pre-computed term frequency lookup table for a given column.

        This method allows you to register a term frequency table in the Splink
        cache for a specific column. This table will then be used during linkage
        rather than computing the term frequency table anew from your input data.

        Args:
            input_data (AcceptableInputTableType): The data representing the term
                frequency table. This can be either a dictionary, pandas dataframe,
                pyarrow table, or a spark dataframe.
            col_name (str): The name of the column for which the term frequency
                lookup table is being registered.
            overwrite (bool, optional): Overwrite the table in the underlying
                database if it exists. Defaults to False.

        Returns:
            SplinkDataFrame: An abstraction representing the registered term
            frequency table.

        Examples:
            ```py
            tf_table = [
                {"first_name": "theodore", "tf_first_name": 0.012},
                {"first_name": "alfie", "tf_first_name": 0.013},
            ]
            tf_df = pd.DataFrame(tf_table)
            linker.table_management.register_term_frequency_lookup(
                tf_df,
                "first_name"
            )
            ```
        """
        ...
    
    def register_labels_table(self, input_data, overwrite=...): # -> SplinkDataFrame:
        ...
    
    def delete_tables_created_by_splink_from_db(self): # -> None:
        ...
    
    def register_table(self, input_table: AcceptableInputTableType, table_name: str, overwrite: bool = ...) -> SplinkDataFrame:
        """
        Register a table to your backend database, to be used in one of the
        splink methods, or simply to allow querying.

        Tables can be of type: dictionary, record level dictionary,
        pandas dataframe, pyarrow table and in the spark case, a spark df.

        Examples:
            ```py
            test_dict = {"a": [666,777,888],"b": [4,5,6]}
            linker.table_management.register_table(test_dict, "test_dict")
            linker.query_sql("select * from test_dict")
            ```

        Args:
            input_table: The data you wish to register. This can be either a dictionary,
                pandas dataframe, pyarrow table or a spark dataframe.
            table_name (str): The name you wish to assign to the table.
            overwrite (bool): Overwrite the table in the underlying database if it
                exists

        Returns:
            SplinkDataFrame: An abstraction representing the table created by the sql
                pipeline
        """
        ...
    


